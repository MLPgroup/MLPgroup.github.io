---
layout: resource
title: "Latent Dirichlet Allocation Basics"

description: This article explores the fundamental aspects of Latent Dirichlet Allocation (LDA), a highly-utilized unsupervised probabilistic technique for topic modeling. It elaborates the core principles of LDA, providing an accessible interpretation of the underlying mathematical concepts that dictate the operation of the model, as well as the training process using Gibbs Sampling with operable illustration. The real-world applications and potential extensions of the LDA model are also explored.

icon: star-o
people:
  - weiyang
---


# 1. Introduction

This article explores the fundamental aspects of Latent Dirichlet Allocation (LDA), a highly-utilized unsupervised probabilistic technique for topic modeling. It elaborates the core principles of LDA, providing an accessible interpretation of the underlying mathematical concepts that dictate the operation of the model, as well as the training process using Gibbs Sampling with operable illustration. The real-world applications and potential extensions of the LDA model are also explored.

<h1>2. Model Assumption</h1>
<ul>
    <li><p>Documents are treated as "bags of words", disregarding the order in which the words appear.</p></li>
    <li><p>Every document is considered as a mixture of these topics, with the contribution of each topic defined by
            a probability.</p></li>
    <li><p>Every document can be described as a mixture of these topics.</p><li>
        <p>For example:</p>
        <ul>
            <li>
                <p>Document 1 Topics: 0.5 * Art + 0.4 * History + 0.1 * Physics</p>
            </li>
            <li>
                <p>Document 2 Topics: 0.7 * Technology + 0.2 * Business + 0.1 * Politics</p>
            </li>
            <li>
                <p>Topic Art: Painting * p1 + Sculpture * p2 + Photography * p3...</p>
            </li>
            <li>
                <p>Topic History: War * p1 + Revolution * p2 + Empire * p3...</p>
            </li>
            <li>
                <p>Topic Physics: Quantum * p1 + Relativity * p2 + Particle * p3...</p>
            </li>
            <li>
                <p>Topic Technology: AI * p1 + Robotics * p2 + Blockchain * p3...</p>
            </li>
            <li>
                <p>Topic Business: Startup * p1 + Corporation * p2 + Investment * p3...</p>
            </li>
            <li>
                <p>Topic Politics: Democracy * p1 + Policy * p2 + Election * p3...</p>
            </li>
        </ul>
    </li>
    </li>
</ul>

<h1>3. Model Intuition</h1>
<p>As stated in section 2, The fundamental idea behind LDA is that every document in 
    the text corpus is a mixture of topics, and each word in a document is attributable to one of these topics.<br>
    By saying a document is "generated" by its given topic distribution, it means that 
    each document is assumed to be produced in the following manner:</p>
<ul>
    <li><p>A document's topic mixture is determined. For example, we might say that Document 1 is 
        70% about "sports", 20% about "politics", and 10% about "economics".</p></li>
    <li><p>Each word in the document is selected. This is done by first choosing a topic based on 
        the document's topic distribution and then choosing a word based on the topic's probability distribution over words. 
        For example, if we choose the "sports" topic for a word in Document 1, 
        we might then choose the word "football" if "football" has a high probability in the "sports" topic.
        And we do this for each "word slot" of the document.</p></li>
</ul>
<p>In this way, the document is "generated" by its given topic distribution, and each word in turn 
    is generated by the topic distribution of its respective document and the word distribution of its assigned topic.<br>
    The task of LDA is to reverse this process. Given a corpus of documents, LDA tries to 
    figure out the topic distributions for each document and the word distributions for each topic 
    that would have most likely resulted in the observed corpus. 
</p>

<h1>4. Theorem Explaination</h1>
<h2>4.1 Formula</h2>
$$
P(\boldsymbol{W},\boldsymbol{Z},\theta,\varphi;\alpha,\beta) =
\prod_{j=1}^{M}P(\theta_j;\alpha) \prod_{i=1}^{K}P(\varphi_i;\beta)
\prod_{t=1}^{N}P(Z_{j,t} \mid \theta_j) P(W_{j,t} \mid \varphi_{Z_{j,t}})
$$

<h2>4.2 Explanation</h2>
<p>The formula represents the joint probability distribution for a LDA model, 
    denoted as \(P(\boldsymbol{W},\boldsymbol{Z},\theta,\varphi;\alpha,\beta)\). Here's what each part means:</p>

<p>\(P(\boldsymbol{W},\boldsymbol{Z},\theta,\varphi;\alpha,\beta)\) is the 
    joint probability of the observed words \(\boldsymbol{W}\), 
    the latent (or hidden) topic assignments \(\boldsymbol{Z}\), 
    the per-document topic proportions \(\theta\), and the per-topic word 
    probabilities \(\varphi\), given the Dirichlet prior parameters \(\alpha\) and \(\beta\).</p>

<p>\(\prod_{j=1}^{M}P(\theta_j;\alpha)\) is the probability of the topic distribution 
    for each document \(j\) under a Dirichlet prior \(\alpha\).</p>

<p>\(\prod_{i=1}^{K}P(\varphi_i;\beta)\) is the probability of the word distribution 
    for each topic \(i\) under a Dirichlet prior \(\beta\).</p>

<p>\(\prod_{t=1}^{N}P(Z_{j,t} \mid \theta_j)\) is the probability of the topic 
    assignments \(Z_{j,t}\) for each word \(t\) in each document \(j\), given 
    the topic distribution \(\theta_j\) of that document.</p>

<p>\(P(W_{j,t} \mid \varphi_{Z_{j,t}})\) is the probability of each word \(W_{j,t}\) 
    in each document \(j\), given the word distribution \(\varphi_{Z_{j,t}}\) of 
    the assigned topic \(Z_{j,t}\) for that word.</p>

<p>The aim of the LDA model is to find values for \(\boldsymbol{Z}\), \(\theta\), 
    and \(\varphi\) that maximize this joint probability, given the observed 
    words \(\boldsymbol{W}\) and the priors \(\alpha\) and \(\beta\). 
    Due to the complexity of this problem, approximation methods like 
    Gibbs sampling or variational inference are often used to estimate these values.
</p>


<h1>5. Training - Gibbs Sampling</h1>
<h2>5.1 Objective</h2>
<p>The principal aim of Gibbs Sampling in LDA is to
    maximize the monochromaticity of topic distribution within documents, and word distribution within topics.
    Implicitly, this implies a desirable model where each document is characterized by one or a limited number of topics, 
    and similarly, each word belongs to a limited number of topics.</p>

<h2>5.2 Procedure</h2>
<p>The following steps outline the typical steps for training:</p>
<ul>
    <li>Initialize the model by randomly assigning topics to each word in each document.</li>
    <li>Iterate sequentially through each word, assume all other topic assignments are correct, 
        and concentrate on the topic distribution of the current word across documents and the topic distribution of its parent document.</li>
    <li>Apply a smoothing operation to prevent zero probabilities. The smoothing values added here correspond to the Dirichlet priors.</li>
    <li>Based on the computed probabilities, decide the topic for the current word via a random draw.</li>
    <li>As the iterations progress, the topic distribution converges to the true underlying distribution.</li>
</ul>

<h2>5.3 Hyperparameters</h2>
<p>The model depends on several critical hyperparameters:</p>
<ul>
    <li>N - The number of iterations.</li>
    <li>K - The number of topics. A higher value allows for more fine-grained topic identification but 
        may lead to overfitting. A lower value yields broader, less specific topics.</li>
    <li>Dirichlet priors - These parameters control the "smoothness" of the topic and word distributions. 
        A higher value results in sparser distributions, i.e., fewer topics heavily contribute to each document 
        and fewer words significantly contribute to each topic. Conversely, a lower value results in denser 
        distributions, i.e., more topics contribute equally to each document, and more words contribute equally to each topic. 
        After initialization, the model may adjust Dirichlet priors as training proceeds 
        (this adjustment may not be present in all code implementations).</li>
</ul>

<h2>5.4 Initialization of Dirichlet Priors</h2>
<p>There are several strategies for initializing the Dirichlet priors:</p>
<ul>
    <li>Random initialization.</li>
    <li>Small value initialization - This strategy generally leads to sparser results.</li>
    <li>Initialization based on prior knowledge.</li>
    <li>Empirical Bayes method - This involves maximizing the marginal likelihood of the data, which tends to perform better than relying on prior knowledge.</li>
    <li>Cross-validation to select the best initialization method.</li>
</ul>

<h2>5.5 Animation</h2>
<!-- <div id ="animationContainer">
    <div class="container">
        <div class="section" id="leftsec">
            <div class="inputContainer">
                <p> Topic Number: </p>
                <input id = "inputN", value = "5">
                <p>alpha: </p>
                <input id = "inputAlpha", value = "0.1">
                <p>beta: </p>
                <input id = "inputBeta", value = "0.1">
                <button id="BtnInput">Modify Hyperparameters</button>
            </div>
            <button id="startWord">Iterate By Word</button>
            <button id="startCorpus">Iterate By Corpus</button>
            <button id="reset">Reset</button>


            <div id="word-container"></div>
            <button id="iterateBtn">Iterate!</button>
        </div>
        <div class="section" id="rightsec">
            <div class="inner">
                <div id="corpusModeDiv">
                    <p>The corpus is now randomly initialized.</p>
                    <p>When you are under "corpus mode", each time you click the iterate button, the model is going to iterate through each word from each document once. This mode is designed for users to notice the convergence faster.</p>
                </div>
                <div id="wordModeDiv_">
                    <p>The corpus is now randomly initialized.</p>
                    <p>When you are under "word mode", each time you click the iterate button, the model is going to be trained for the current word. This mode is designed for user to learn the underlying details of model training.</p>
                </div>
                <div id="wordModeDiv">
                    <p id = "sentence_1"></p>
                    <p>Step 1: Unlabel the word with topic and continue;</p>
                    <p>Step 2: Iterate through topic, take topic 1 as an example;</p>
                    <p>Step 3: For current document, calculate the ratio of the occrance of topic 1 compared to the corpus with alpha added for smoothing, by formula:</p>
                    <p>(occrance of current topic in current document + α) / (length of current topic + α * number of topics)</p>
                    <p id = "sentence_2"></p>
                    <p>Step 4: For the current word, calculate the ratio of the occrance of topic 1 compared to the corpus with beta added for smoothing, by formula:</p>
                    <p>(occrance of current word of current topic + β) / (total occrance of current word + β * size of vocabulary)</p>
                    <p id = "sentence_3"></p>
                    <p>Step 5: Multiply the above two probabilities and append them to the distribution list:</p>
                    <p id="sentence_4"></p>
                    <p>Step 6: Iterate Step 2 - 5 for each topic, resulting with a raw distribution:</p>
                    <div id="distribution_1"></div>
                    <p>Step 7: Normalize the distribution:</p>
                    <div id="distribution_2"></div>
                    <p>Step 8: Randomly draw from the distribution:</p>
                    <p id="sentence_5"></p>
                </div>
            </div>
            
        </div>
    </div>
<script src="{{ site.base }}/js/gibbs.js"></script>
</div> -->
<iframe src="{{ site.base }}/img/LDA/gibbs.html" height="1000" width="1000" title="LDA Animation"></iframe>




<h1>6. Basic Applications of LDA</h1>
<h2>6.1 Topic Modeling</h2>
<p>The basic implementation of LDA, as stated above, takes a set of documents as input, and is able to 
    give the topic distribution for each document. However, LDA doesn't output nominal topics directly, 
    which means that the topics within the result don't have any nominal meanings themselves. 
    By studying the words that compose each topic, it is possible to assign
    names and labels to the topics, but this requires further steps and techniques.
</p>
<h2>6.2 Document Clustering</h2>
<p>LDA can be used to cluster large collections of text documents into
    topics, which can help with tasks such as information retrieval,
    document classification, and recommendation systems.</p>
<h2>6.3 Content-based recommendation</h2>
<p>LDA can be used to identify the topics that a user is interested in
    based on their past behavior or preferences, and then recommend similar
    content that matches those topics.</p>
<h2>6.4 Sentiment Analysis</h2>
<p>LDA can be used to analyze the sentiment of a piece of text by
    identifying the topics that are most commonly associated with positive
    or negative sentiment.</p>


<h1>7. Extended Applications of LDA</h1>
<p>As described so far, LDA is based on the distribution of words in a
    corpus of text, often it is directly applied to text objects, but it
    can also handle other problems with proper feature transformations.</p>
<h2>7.1 Fraud Detection</h2>
<ul>
    <li>
        <p>There are features for transactions, such as date, time, type,
            amount, andlocation.</p>
    </li>
    <li>
        <p>Transfer them to BOG models, such as “time: 9:00 am”, “amount:
            $1000”, “location: Champaign, IL”Each of the above features is treated
            as single word in the BOG model.</p>
    </li>
    <li>
        <p>Applying LDA gives the distribution of topics mixture.</p>
    </li>
    <li>
        <p>Cluster the transactions based on the topic
            distribution.</p>
    </li>
    <li>
        <p>Human effort to recognize the topic / cluster that’s
            suspicious.</p>
    </li>
    <li>
        An in-depth example could be found 
        <a href="https://www.sciencedirect.com/science/article/pii/S1877050918314753" target="_blank">here.</a>
    </li>
</ul>
<h2>7.2 Medical diagnosis</h2>
<ul>
    <li>
        <p>Data comes from the patients’ medical history and case symptoms,
            such as:"cough," asthma”, "breathlessness”, "wheezing”.</p>
    </li>
    <li>
        <p>Also the medicine used, and lab results can be part of data,
            there are countless features with proper feature processing.</p>
    </li>
    <li>
        <p>Apply LDA and retrieve topic distributions for each
            patient.</p>
    </li>
    <li>
        <p>From the result human effort is required for the recognition of
            high probabilities for specific medical conditions according to the
            topic distributions.</p>
    </li>
    <li>
        An in-depth example could be found 
        <a href="https://www.sciencedirect.com/science/article/pii/S1532046416000253" target="_blank">here.</a>
    </li>
</ul>

<h1>8. Additional Resources</h2>
<ul>
    <li>
        <a href="https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094">
            A helpful tutorial by loana.
        </a>
    </li>
    <li>
        <a href="https://link.springer.com/article/10.1007/s11042-018-6894-4">
            A frequently cited paper presenting overview, trends and development of LDA. 
        </a>
    </li>
    
</ul>